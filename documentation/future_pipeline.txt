# -*- org -*-
#+INFOJS_OPT: view:content toc:t ltoc:t mouse:#dddddd
#+OPTIONS:     H:5

Future pipeline

* Requirements
** What to store
# <<what_to_store>>
*** Metadata (AKA "Tracking")
**** Organisations
     - name
     - description
     - creation time/date stamp
**** People (experiment owners)
     - use CRS id (eg. kmr44) or email address
     - first name
     - last last
     - organisation reference
     - creation time/date stamp
**** Projects (experiments)
     - project name
       - autogenerated from user name? eg. kmr44-12
       - or allow user to create a unique name?
     - type
     - organism?
     - creation time/date stamp
**** Samples
     - sample name
       - autogenerated from project name? eg. kmr44-12-34
     - description
     - molecule type (DNA, RNA, ...)
     - barcode
     - tissue
     - organism
     - ecotype
     - protocol used to create the sample?
     - project reference
     - creation time/date stamp
**** Barcodes
     - Store barcode sequence and identifier 'A', 'B', ... 
**** Sequencing runs
     - identifier autogenerated from sample name? eg. kmr44-12-34-run4
     - sample reference
     - because of multiplexing, one sequencing run can contain several samples
     - creation time/date stamp
**** Sample run
     - Barcodes need to be attached to sample/sequencing run combination since
       the barcode may need to change if a sample is rerun or some runs will
       have a barcode while others don't
     - this corresponds to the table that joins the sequencingrun and sample
       tables
**** Organisms
     - genus
     - species
     - common name?
     - abbreviation?
     - NCBI taxonomy ID if available?
     - creation time/date stamp
     - initial possibilities:
       - A. thaliana
       - C. reinhardtii
       - Z. maize
       - L. esculentum
       - L. pimpinellifolium
       - L. pennellii
       - O. sativa
       - S. pombe
**** Ecotypes
     - Should users be able to add ecotypes?
     - Arabidopsis examples:
       - Col
       - Ler
       - WS
       - C24
       - Cvi
**** Tissue types
     - Should users be able to add tissue types?
     - Arabidopsis examples:
       - unopened flowers (stage 0-12)
       - open flowers (stage 13)
       - young siliques (<7 dpf)
       - mature siliques (>7 dpf)
       - young leaves (<14 days)
       - mature leaves (>14 days)
       - vegetative meristem
       - floral meristem
       - roots (including meristem)
       - seedlings (roots, cotyledons, leaves, and meristem)
       - cauline leaves
       - stem
     - Chlamydomonas examples:
       - vegetative cells
       - gametes
**** Genotype
     Arabidopsis examples:
     - nrpd1a-4
     - ago4-2 ago6-1
     - RDR2p::RDR2cDNA-GFP
**** Biological source
     - ?
**** Biological condition?
     Or just rely on the description?
**** Sample treatment
     Some possibilities:
     - salt
     - 5-azacytidine
**** Sample fractionation
     Some possibilities:
     - cell sorting
     - nuclear fraction
*** Analysis progress (AKA "Pipeline")
**** Analysis types (table: process_conf)
     - Configuration for possible analysis steps
     - Store in the database
     - Need to configure the needed input data types
     - Store a Perl module name that contains the code to implement this
       analysis
     - For possible analysis types, see [[Pipeline steps]] Below
**** Analysis run (table: pipeprocess)
     - Information about one analysis step
     - Needs:
       - reference to an analysis type
       - reference input and output data
       - a status ('not_started', 'started', 'queued', 'finished')
       - a job identifier if a (Torque) job has been started
       - time stamps for: time_queued, time_started, time_finished
       - a description, probably derived from the analysis type
**** Analysis results (table: pipedata)
     - Track file locations, don't store results
       - Store sequence in fasta/fastq files
     - Store the file content type (eg. "raw_small_rna_reads")
     - Store the format type (eg. "fasta")
*** Do we need to store old versions?
** User interface
*** Functions
**** Add organisation
**** Add user
     - A user must have an organisation
**** Add a project (experiment)
     - Needs an owner (user)
**** Create a sequencing run
     The run needs a unique ID
**** Add a sample
     We need to be able to associate a sample with a sequencing run and when
     we do, we need to specify a barcode (if any)
**** Removal of:
     - projects
       - perhaps just flag as deleted?
     - samples
       - perhaps just flag as deleted?
     - users (only by admin?)
     - organisations (only by admin?)
**** Cancel sequencing run?
*** View
**** User experiments, samples, sequencing runs
**** Check status of sequencing runs
     Status could be:
     - created
     - sent to sequencing centre
     - received from sequencing centre: good quality
     - received from sequencing centre: bad quality/failed run
**** Check status of analysis
     - 'not_started'
     - 'started'
     - 'queued'
     - 'finished'
**** Get links to all files (fastq, fasta, summary, alignment)
     - download
     - download as zip/compressed?
     - view in browser (for small files like summaries)
     - view graphically (eg. for summaries)
**** Analysis tools
***** Logging
****** date-time-user stamps
****** audit trail
***** Statistics/summaries/counts
****** sequences lost/reason at each stage
***** Graphs of size distributions (and other things)
***** Current CGI scripts
***** Frank's tools
***** Tom's DE analysis
**** Management overview / reports
***** samples by month, species, type etc
***** time from submission to receipt of data
***** total reads, sequences lost by reason

** Analysis/pipeline steps
# <<Pipeline steps>>
*** Where does the pipeline start?
**** Intensity files
     - we may have to take on this part of the processing
     - do we want to e.g try other algorithms?
**** FastQ files
**** Retrieval of files from the sequencing centres
     Hopefully we can retreive the fastq/intensity files automatically and
     start processing through the pipeline automatically 
*** Remove adaptors/demultiplex
    - generates fasta files
    - due to multiplexing,  there may be many fasta files for one fastq, each
      of which corresponds to one sample
*** Truncation of sequencing
    - Andy isn't sequencing small RNA and needs to truncate each sequence to
      25 bases before further processing - no adaptor removal needed
    - This may need de-multiplexing
*** Alignments
    Use ssaha or exonerate and record the start, end, chromosome and
    organism
    - Genome
    - RNA databases
*** Process alignment
    Could be done by parsing the alignment program output as it arrives to
    save a pipeline step and to save disk space
**** generate GFF
**** generate FASTA of those features that match the genome
*** Alignment processing
    - Compare the alignments that match the genomic DNA and the alignments
      against the RNA databases 
    - Generate an alignment file (in GFF3 probably) and a FASTA file
      containing only those sequences that align to genome and don't also
      match/align to the RNA databases
*** Locus definition
** Where does the pipeline end?
*** Load into GBrowse
*** Data for upload to public databases e.g. minseqe, GEO etc
* Database structure
** Chado?
   Gives us
   - Organism
   - Stock module?
   - Genome feature
   - CV terms
     - Anatomy ontology for plants: http://www.plantontology.org/
** Other tables
   We'll need a table for each of the data types mentioned in the [[what_to_store][What to store]]
   section
* Implementation notes
** Running jobs
   Basic ideas
   - we have a bunch of analysis programs to run and a bunch of
     files of various types
   - some files are generated from others by analysis programs (eg. adaptor
     removal takes a fastq file and makes a fasta file)
   - there are dependencies: we might need to make a summary file from a fasta
     file but that fasta file must first be created by the adaptor removal
     program
   - some files aren't generated for an analysis (eg. fastq or SRF files)
   - some files need no processing (eg. a summary file)
*** Deciding what jobs to run and when to run them
    A process needs to regularly examine the database and decide what jobs to
    run.  It needs to query the database for the current files and compare
    that result to the possible analyses.
**** Implementation possibilities
    - have a server process that is always running
       - efficient as it can keep state cached in memory
       - harder to manage (needs restarting if a server restarts)
    - have a cron job that starts a process regularly to check for jobs
       - less efficient as it needs to scan the database for jobs to run
       - easier to manage and to program/debug
*** Dependencies
    What to run next depends on what files we currently have, their types
    and whether the next analysis has been run.
**** Analysis example 1 - type "adaptor_removal"
     - input:
       - a file in format "fastq"
       - contents is sequence for molecule type "rna"
       - from a sequencing run for a sample that has type "small_rna"
       - and was not generated by an analysis
     - output:
       - a file in format "fasta" with contents of type "rna_reads"
**** Analysis example 2 - type "base_count_summary"
     - input:
       - a file in format "fasta"
       - contents of type "rna_reads" or "dna_reads"
     - output:
       - a summary file, file format "read_summary"
**** Analysis example 3 - type "align_to_genome_gff"
     - input:
       - a file in format "fasta"
       - contents of type "rna_reads" or "dna_reads"
     - output:
       - a gff file containing the coordinates
       - file format "gff3"
       - contents of type "aligned_rna_reads" or "aligned_dna_reads"
**** Dependency storage
    We must encode the dependencies and logic somewhere, either:
    - in a file or perl code
    - in the database
* Tasks/steps in the pipeline
*** Load Gbrowse
*** "Target prediction" - what's that?


